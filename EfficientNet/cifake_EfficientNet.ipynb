{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93379f53",
   "metadata": {},
   "source": [
    "# Phase 1 — Synthetic Image Detector (EfficientNet)\n",
    "This notebook trains an **EfficientNet**-based classifier to detect **real vs AI-generated (synthetic)** images using the **CIFAKE** dataset.\n",
    "\n",
    "**Outputs**\n",
    "- Train/Val/Test metrics: Accuracy, Precision, Recall, F1\n",
    "- Confusion Matrix\n",
    "- Visual explainability: **Grad-CAM** + **Saliency maps**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Setup =====\n",
    "# If running on Colab, ensure GPU: Runtime → Change runtime type → GPU\n",
    "\n",
    "!pip -q install kagglehub==0.3.10 scikit-learn matplotlib tqdm\n",
    "\n",
    "import os, re, math, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81bd79",
   "metadata": {},
   "source": [
    "## 1) Download CIFAKE dataset via kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "DATA_ROOT = Path(path)\n",
    "DATA_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a11a9",
   "metadata": {},
   "source": [
    "## 2) Locate dataset folders\n",
    "\n",
    "The dataset usually contains `train/` and `test/` folders, each with class subfolders.\n",
    "This helper tries to locate a valid ImageFolder root automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_imagefolder_root(root: Path):\n",
    "    \"\"\"Return a folder that can be read by torchvision.datasets.ImageFolder.\"\"\"\n",
    "    root = Path(root)\n",
    "    # Common patterns\n",
    "    candidates = [\n",
    "        root / \"train\",\n",
    "        root / \"Train\",\n",
    "        root / \"training\",\n",
    "        root / \"Training\",\n",
    "        root / \"data\" / \"train\",\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists() and c.is_dir():\n",
    "            # must contain at least 2 class dirs\n",
    "            class_dirs = [p for p in c.iterdir() if p.is_dir()]\n",
    "            if len(class_dirs) >= 2:\n",
    "                return c, (root / \"test\" if (root / \"test\").exists() else root / \"Test\")\n",
    "    # Fallback: search for first directory that looks like ImageFolder root\n",
    "    for d in root.rglob(\"*\"):\n",
    "        if d.is_dir():\n",
    "            class_dirs = [p for p in d.iterdir() if p.is_dir()]\n",
    "            if len(class_dirs) >= 2:\n",
    "                # verify it has images underneath\n",
    "                has_img = any(d.rglob(\"*.png\")) or any(d.rglob(\"*.jpg\")) or any(d.rglob(\"*.jpeg\"))\n",
    "                if has_img:\n",
    "                    # also try to find sibling test folder\n",
    "                    sibling_test = d.parent / \"test\"\n",
    "                    return d, (sibling_test if sibling_test.exists() else None)\n",
    "    raise FileNotFoundError(\"Could not find an ImageFolder-compatible dataset root under: \" + str(root))\n",
    "\n",
    "train_root, test_root_guess = find_imagefolder_root(DATA_ROOT)\n",
    "print(\"Train root:\", train_root)\n",
    "print(\"Test root (guess):\", test_root_guess)\n",
    "print(\"Class folders:\", [p.name for p in train_root.iterdir() if p.is_dir()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265fe56a",
   "metadata": {},
   "source": [
    "## 3) Transforms & DataLoaders\n",
    "\n",
    "CIFAKE images are **32×32**, but EfficientNet is pretrained on ImageNet and expects larger inputs.\n",
    "We **upsample to 224×224** (standard for EfficientNet-B0) and use ImageNet normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6628dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 0  # set 0 to avoid Py3.12/Colab multiprocessing shutdown errors\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "\n",
    "# ImageNet stats (for pretrained EfficientNet)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.02),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "full_train_ds = datasets.ImageFolder(train_root, transform=train_tfms)\n",
    "\n",
    "# Optional: load official test split if present\n",
    "test_root = None\n",
    "if test_root_guess is not None and Path(test_root_guess).exists():\n",
    "    # try to ensure it is ImageFolder-compatible\n",
    "    try:\n",
    "        _ = datasets.ImageFolder(test_root_guess)\n",
    "        test_root = Path(test_root_guess)\n",
    "    except Exception:\n",
    "        test_root = None\n",
    "\n",
    "test_ds = datasets.ImageFolder(test_root, transform=test_tfms) if test_root else None\n",
    "\n",
    "class_names = full_train_ds.classes\n",
    "num_classes = len(class_names)\n",
    "print(\"Classes:\", class_names, \"num_classes=\", num_classes)\n",
    "print(\"Train samples:\", len(full_train_ds))\n",
    "if test_ds:\n",
    "    print(\"Test samples:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train/Val split from train folder (e.g., 90/10)\n",
    "VAL_RATIO = 0.1\n",
    "n_total = len(full_train_ds)\n",
    "n_val = int(n_total * VAL_RATIO)\n",
    "n_train = n_total - n_val\n",
    "\n",
    "train_ds, val_ds = random_split(\n",
    "    full_train_ds,\n",
    "    [n_train, n_val],\n",
    "    generator=torch.Generator().manual_seed(SEED),\n",
    ")\n",
    "\n",
    "# Important: val_ds should use deterministic transforms (no random aug)\n",
    "# We can override transform by wrapping the underlying dataset with a new transform via a small helper.\n",
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __len__(self): \n",
    "        return len(self.subset)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        # subset already applied train_tfms; we need to reload raw image for true override.\n",
    "        # So instead, rebuild val dataset from original ImageFolder with indices.\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Rebuild val dataset properly using indices on the original ImageFolder\n",
    "val_indices = val_ds.indices\n",
    "train_indices = train_ds.indices\n",
    "\n",
    "base_ds = datasets.ImageFolder(train_root, transform=test_tfms)  # deterministic for val\n",
    "\n",
    "class SubsetFromBase(torch.utils.data.Dataset):\n",
    "    def __init__(self, base, indices):\n",
    "        self.base = base\n",
    "        self.indices = indices\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, i):\n",
    "        return self.base[self.indices[i]]\n",
    "\n",
    "val_ds = SubsetFromBase(base_ds, val_indices)\n",
    "train_ds = SubsetFromBase(datasets.ImageFolder(train_root, transform=train_tfms), train_indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY) if test_ds else None\n",
    "\n",
    "len(train_loader), len(val_loader), (len(test_loader) if test_loader else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26b544",
   "metadata": {},
   "source": [
    "## 4) Model — EfficientNet-B0 (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3379a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained EfficientNet-B0 and replace classifier head\n",
    "weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "model = models.efficientnet_b0(weights=weights)\n",
    "\n",
    "# Replace last layer for binary classification (or multi-class if dataset differs)\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "# Scheduler (cosine)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Mixed precision\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825971c",
   "metadata": {},
   "source": [
    "## 5) Training & evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35821a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_logits(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_y = [], []\n",
    "    for x, y in tqdm(loader, desc=\"Predict\", leave=False):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "        all_y.append(y.detach().cpu())\n",
    "    return torch.cat(all_logits, dim=0), torch.cat(all_y, dim=0)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"binary\" if num_classes==2 else \"macro\", zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=\"binary\" if num_classes==2 else \"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"binary\" if num_classes==2 else \"macro\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x, y in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_true = [], []\n",
    "    for x, y in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "        all_true.append(y.detach().cpu())\n",
    "\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_true).numpy()\n",
    "    metrics = compute_metrics(y_true, y_pred)\n",
    "    return running_loss / len(loader.dataset), metrics, (y_true, y_pred)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.imshow(cm)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(class_names)), class_names)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5a3eb",
   "metadata": {},
   "source": [
    "## 6) Train EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5  # CIFAKE is big; start with 3–5, then increase if time allows\n",
    "best_f1 = -1\n",
    "best_path = \"best_efficientnet_cifake.pt\"\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"val_f1\": []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_metrics, (y_true, y_pred) = eval_one_epoch(model, val_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_metrics[\"accuracy\"])\n",
    "    history[\"val_f1\"].append(val_metrics[\"f1\"])\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \" \n",
    "          f\"acc={val_metrics['accuracy']:.4f} prec={val_metrics['precision']:.4f} rec={val_metrics['recall']:.4f} f1={val_metrics['f1']:.4f} \" \n",
    "          f\"| time={(time.time()-t0):.1f}s\")\n",
    "\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(\"  ✓ Saved best model ->\", best_path)\n",
    "\n",
    "print(\"Best val F1:\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss curves\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.plot(history[\"val_f1\"], label=\"val_f1\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation metrics\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb725f6",
   "metadata": {},
   "source": [
    "## 7) Final evaluation (Val + Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Validation report\n",
    "val_logits, val_y = predict_logits(model, val_loader)\n",
    "val_pred = val_logits.argmax(dim=1).numpy()\n",
    "val_true = val_y.numpy()\n",
    "\n",
    "print(\"VAL METRICS:\", compute_metrics(val_true, val_pred))\n",
    "print(\"\\nClassification report (VAL):\\n\", classification_report(val_true, val_pred, target_names=class_names, zero_division=0))\n",
    "cm_val = plot_confusion_matrix(val_true, val_pred, class_names)\n",
    "\n",
    "# Test report (if available)\n",
    "if test_loader is not None:\n",
    "    test_logits, test_y = predict_logits(model, test_loader)\n",
    "    test_pred = test_logits.argmax(dim=1).numpy()\n",
    "    test_true = test_y.numpy()\n",
    "\n",
    "    print(\"\\nTEST METRICS:\", compute_metrics(test_true, test_pred))\n",
    "    print(\"\\nClassification report (TEST):\\n\", classification_report(test_true, test_pred, target_names=class_names, zero_division=0))\n",
    "    cm_test = plot_confusion_matrix(test_true, test_pred, class_names)\n",
    "else:\n",
    "    print(\"No separate test split found. (Using only Train/Val split.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601a352",
   "metadata": {},
   "source": [
    "## 8) Visual Explainability\n",
    "We generate:\n",
    "- **Grad-CAM** heatmaps (what spatial regions drove the decision)\n",
    "- **Saliency maps** (input gradients)\n",
    "\n",
    "We'll visualize a few samples from the **validation** set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf68ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: de-normalize for visualization\n",
    "inv_norm = transforms.Normalize(\n",
    "    mean=[-m/s for m, s in zip(IMAGENET_MEAN, IMAGENET_STD)],\n",
    "    std=[1/s for s in IMAGENET_STD]\n",
    ")\n",
    "\n",
    "def tensor_to_img(t):\n",
    "    t = t.detach().cpu()\n",
    "    t = inv_norm(t)\n",
    "    t = torch.clamp(t, 0, 1)\n",
    "    return t.permute(1,2,0).numpy()\n",
    "\n",
    "# Pick some samples\n",
    "def get_batch(loader, n=8):\n",
    "    x, y = next(iter(loader))\n",
    "    return x[:n], y[:n]\n",
    "\n",
    "x_vis, y_vis = get_batch(val_loader, n=8)\n",
    "x_vis.shape, y_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64840016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Grad-CAM implementation (generic) =====\n",
    "# Works for torchvision EfficientNet: we use the last conv block in model.features\n",
    "target_layer = model.features[-1]\n",
    "\n",
    "activations = None\n",
    "gradients = None\n",
    "\n",
    "def forward_hook(module, inp, out):\n",
    "    global activations\n",
    "    activations = out\n",
    "\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    global gradients\n",
    "    gradients = grad_out[0]\n",
    "\n",
    "# Register hooks\n",
    "_ = target_layer.register_forward_hook(forward_hook)\n",
    "_ = target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "def grad_cam(model, x, class_idx=None):\n",
    "    \"\"\"Return Grad-CAM heatmap (H,W) for a single image tensor x (1,C,H,W).\"\"\"\n",
    "    model.eval()\n",
    "    global activations, gradients\n",
    "    activations, gradients = None, None\n",
    "\n",
    "    x = x.to(device)\n",
    "    logits = model(x)\n",
    "    if class_idx is None:\n",
    "        class_idx = logits.argmax(dim=1).item()\n",
    "\n",
    "    score = logits[:, class_idx].sum()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    score.backward(retain_graph=True)\n",
    "\n",
    "    # activations: (1, C, h, w), gradients: (1, C, h, w)\n",
    "    weights = gradients.mean(dim=(2,3), keepdim=True)  # (1,C,1,1)\n",
    "    cam = (weights * activations).sum(dim=1, keepdim=False)  # (1,h,w)\n",
    "    cam = torch.relu(cam)\n",
    "    cam = cam - cam.min()\n",
    "    cam = cam / (cam.max() + 1e-8)\n",
    "    cam = cam.squeeze(0).detach().cpu().numpy()\n",
    "    return cam, logits.detach().cpu()\n",
    "\n",
    "def overlay_cam(img, cam, alpha=0.5):\n",
    "    # img: H,W,3 in [0,1], cam: h,w in [0,1]\n",
    "    cam_resized = torch.tensor(cam).unsqueeze(0).unsqueeze(0)\n",
    "    cam_resized = torch.nn.functional.interpolate(cam_resized, size=img.shape[:2], mode=\"bilinear\", align_corners=False)\n",
    "    cam_resized = cam_resized.squeeze().numpy()\n",
    "    overlay = (1 - alpha) * img + alpha * np.stack([cam_resized]*3, axis=-1)\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    return overlay, cam_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbeacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grad-CAM for a few samples\n",
    "model.eval()\n",
    "\n",
    "n_show = 6\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(n_show):\n",
    "    x1 = x_vis[i:i+1]\n",
    "    y1 = y_vis[i].item()\n",
    "\n",
    "    cam, logits = grad_cam(model, x1, class_idx=None)\n",
    "    probs = torch.softmax(logits, dim=1).squeeze(0).numpy()\n",
    "    pred = int(np.argmax(probs))\n",
    "\n",
    "    img = tensor_to_img(x_vis[i])\n",
    "    overlay, cam_big = overlay_cam(img, cam, alpha=0.5)\n",
    "\n",
    "    plt.subplot(n_show, 3, 3*i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Original\\nTrue: {class_names[y1]}\")\n",
    "\n",
    "    plt.subplot(n_show, 3, 3*i + 2)\n",
    "    plt.imshow(cam_big)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Grad-CAM heatmap\")\n",
    "\n",
    "    plt.subplot(n_show, 3, 3*i + 3)\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Overlay\\nPred: {class_names[pred]} ({probs[pred]:.2f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Saliency map (input gradients) =====\n",
    "def saliency_map(model, x, class_idx=None):\n",
    "    \"\"\"Return saliency map (H,W) for a single image tensor x (1,C,H,W).\"\"\"\n",
    "    model.eval()\n",
    "    x = x.to(device)\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "    logits = model(x)\n",
    "    if class_idx is None:\n",
    "        class_idx = logits.argmax(dim=1).item()\n",
    "\n",
    "    score = logits[:, class_idx].sum()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    score.backward()\n",
    "\n",
    "    # gradient magnitude across channels\n",
    "    sal = x.grad.detach().abs().max(dim=1)[0]  # (1,H,W)\n",
    "    sal = sal - sal.min()\n",
    "    sal = sal / (sal.max() + 1e-8)\n",
    "    return sal.squeeze(0).cpu().numpy(), logits.detach().cpu()\n",
    "\n",
    "# Visualize saliency for a few samples\n",
    "n_show = 6\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(n_show):\n",
    "    x1 = x_vis[i:i+1]\n",
    "    y1 = y_vis[i].item()\n",
    "\n",
    "    sal, logits = saliency_map(model, x1, class_idx=None)\n",
    "    probs = torch.softmax(logits, dim=1).squeeze(0).numpy()\n",
    "    pred = int(np.argmax(probs))\n",
    "\n",
    "    img = tensor_to_img(x_vis[i])\n",
    "\n",
    "    plt.subplot(n_show, 2, 2*i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Original\\nTrue: {class_names[y1]} | Pred: {class_names[pred]} ({probs[pred]:.2f})\")\n",
    "\n",
    "    plt.subplot(n_show, 2, 2*i + 2)\n",
    "    plt.imshow(sal)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Saliency map\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984dfc61",
   "metadata": {},
   "source": [
    "## 9) Notes for the report\n",
    "- Include your metric tables (Val/Test) and confusion matrix screenshots.\n",
    "- Add several Grad-CAM / Saliency examples for both **real** and **fake** images.\n",
    "- Briefly describe what the model seems to focus on (edges, textures, backgrounds, etc.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
